{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:52:43.486285Z",
     "start_time": "2024-07-05T19:52:42.005456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules import AthenaQueryExecutor, SqlRender\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from extra.config import awsconfig\n",
    "from extra.utils import (extract, load, utils)\n",
    "from extra.constants import (trx_columns, data_id_pago, name_columns_masivo)\n",
    "from extra import transforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:52:43.490072Z",
     "start_time": "2024-07-05T19:52:43.487759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfil AWS configurado: NEQUI-DataBusinessAnalyst-Role-044879804046\n"
     ]
    }
   ],
   "source": [
    "awsconfig.aws_config_profile('NEQUI-DataBusinessAnalyst-Role-044879804046')\n",
    "# # Establecer el perfil que deseas usar\n",
    "# profile_name = 'NEQUI-DataBusinessAnalyst-Role-044879804046'\n",
    "# # Configurar variable de entorno para AWS_PROFILE\n",
    "# os.environ['AWS_PROFILE'] = profile_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo para crear archivo historico del masivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:52:43.495675Z",
     "start_time": "2024-07-05T19:52:43.490894Z"
    }
   },
   "outputs": [],
   "source": [
    "def creacion_archivo_historico(df_hist):\n",
    "    df_hist['Fecha creación reclamo'] = pd.to_datetime(df_hist['created_at']).apply(lambda x: x.date().strftime('%d/%m/%y'))\n",
    "    df_hist['Fecha escalada a Opercaiones'] = None\n",
    "    df_hist['Rad. Zendesk'] = df_hist['ticket_id']\n",
    "    df_hist['Núm. documento afectad@'] = df_hist['numero_de_documento']\n",
    "    df_hist['Núm. cuenta suplantada'] = df_hist['numero_de_cuenta_870']\n",
    "    df_hist['Nombre completo'] = df_hist['nombre_del_solicitante']\n",
    "    df_hist['Producto'] = df_hist['numero_de_cuenta_870'].apply(lambda x: 'Cuenta' if(str(x)[:3]=='870') else 'credito')\n",
    "    df_hist['Transacción Datacrédito'] = None\n",
    "    df_hist['Rad. Zendesk de seguimiento'] = None\n",
    "    df_hist['Fecha gestión'] = datetime.datetime.now().date().strftime('%d/%m/%y')\n",
    "    df_hist['Equipo solución'] = 'Operaciones'\n",
    "    df_hist['CIERRE APR'] = df_hist['Error Description']\n",
    "    df_hist['Fecha respuesta reclamo'] = None\n",
    "    df_hist['concat 1'] = df_hist['Rad. Zendesk'].apply(lambda x: f\"Suplantación de identidad no reconocida en ticket {x}\")\n",
    "    df_hist['concat 2'] = df_hist.apply(lambda x: f\"Se baja reporte en Datacredito para usuario con CC {x['numero_de_documento']}\", axis=1)\n",
    "    cols = ['Fecha creación reclamo', 'Fecha escalada a Opercaiones', 'Rad. Zendesk', \n",
    "            'Núm. documento afectad@', 'Núm. cuenta suplantada', 'Nombre completo',\n",
    "            'Producto', 'Transacción Datacrédito', 'Rad. Zendesk de seguimiento', \n",
    "            'Fecha gestión', 'Equipo solución', 'CIERRE APR', 'Fecha respuesta reclamo',\n",
    "            'concat 1', 'concat 2']\n",
    "    return df_hist[cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del APR (automatico en folder input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:52:43.542874Z",
     "start_time": "2024-07-05T19:52:43.496742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['APRCIERREMLZ2025051300.xlsx']\n",
      "Archivo unico\n",
      "File nombrado OK\n",
      "Index(['File Name                     ', 'Upload Date ', 'Status    ',\n",
      "       'Error Description                                 ',\n",
      "       'Account Number  ', 'Transfer Account Number', 'Reason Code  ',\n",
      "       'Closure Notes                                     ', 'Tran Id   ',\n",
      "       'Tran Date   ', 'User Part Tran Code  ',\n",
      "       'Tran particulars                                  '],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65 entries, 0 to 64\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                              Non-Null Count  Dtype         \n",
      "---  ------                                              --------------  -----         \n",
      " 0   File Name                                           65 non-null     object        \n",
      " 1   Upload Date                                         65 non-null     datetime64[ns]\n",
      " 2   Status                                              65 non-null     object        \n",
      " 3   Error Description                                   65 non-null     object        \n",
      " 4   Account Number                                      65 non-null     int64         \n",
      " 5   Transfer Account Number                             65 non-null     object        \n",
      " 6   Reason Code                                         65 non-null     int64         \n",
      " 7   Closure Notes                                       65 non-null     object        \n",
      " 8   Tran Id                                             65 non-null     object        \n",
      " 9   Tran Date                                           65 non-null     object        \n",
      " 10  User Part Tran Code                                 65 non-null     object        \n",
      " 11  Tran particulars                                    65 non-null     object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(9)\n",
      "memory usage: 6.2+ KB\n",
      "None\n",
      "File Name\n",
      "Status\n",
      "Error Description\n",
      "Closure Notes\n",
      "Tran Id\n",
      "User Part Tran Code\n",
      "Tran particulars\n",
      "\n",
      "*****\n",
      "Registros del archivo de entrada APR enviado por Operaciones: (65, 12)\n"
     ]
    }
   ],
   "source": [
    "# 1. Listar archivos .xlsx que hay en el folder input/ y leer el df (Solo 1 archivo)\n",
    "\n",
    "# Obtener el directorio padre\n",
    "dir_project = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Directorio input data de insumo\n",
    "dir_data = os.path.join(dir_project, 'co_gestionFraude_eliminacionMasivaExperian/data/input')\n",
    "# Patron regex para validar nombre del archivo\n",
    "regex_partition = r'^APRCIERRE[^\\d]*(\\d{8})(\\d{2}).*.xlsx$'\n",
    "\n",
    "# Lista para almacenar los nombres de archivos Excel en el directorio\n",
    "archivos_excel = []\n",
    "\n",
    "# Iterar sobre los archivos en el directorio\n",
    "for archivo in os.listdir(dir_data):\n",
    "    if archivo.endswith('.xlsx'):  # Puedes ajustar esto según el formato de tus archivos Excel\n",
    "        archivos_excel.append(archivo)\n",
    "        print(archivos_excel)\n",
    "\n",
    "# Verificar la cantidad de archivos Excel\n",
    "cantidad_archivos = len(archivos_excel)\n",
    "\n",
    "if cantidad_archivos == 0:\n",
    "    print(\"No se encontraron archivos Excel en el directorio.\")\n",
    "elif cantidad_archivos == 1:\n",
    "\n",
    "    print(\"Archivo unico\")\n",
    "    #   Leer el archivo Excel\n",
    "        # Validar el nombre del archivo con la expresión regular\n",
    "    if re.match(regex_partition, archivos_excel[0]):\n",
    "            file_input_path = os.path.join(dir_data, archivos_excel[0])\n",
    "            print(\"File nombrado OK\")\n",
    "            df_raw = pd.read_excel(file_input_path)\n",
    "    else:\n",
    "         print(\"Archivo mal nombrado\")\n",
    "else:\n",
    "     print(\"Hay más de un archivo\")\n",
    "\n",
    "\n",
    "df = df_raw.copy()\n",
    "print(df.columns)\n",
    "\n",
    "print(df.info())\n",
    "df.columns = [i.strip() for i in df.columns]\n",
    "for i in df.columns:\n",
    "    if i not in (\"Upload Date\", \"Account Number\", \"Reason Code\", \"Tran Date\", \"Transfer Account Number\"):\n",
    "        print(i)\n",
    "        df[i] = df[i].apply(lambda x: x.strip())\n",
    "\n",
    "print(f\"\\n*****\\nRegistros del archivo de entrada APR enviado por Operaciones: {df.shape}\")\n",
    "\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:52:43.546809Z",
     "start_time": "2024-07-05T19:52:43.544863Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_columns_str = df.select_dtypes(include=['object', 'string'])\n",
    "\n",
    "# # Obtener los nombres de las columnas seleccionadas\n",
    "# name_columns_str = df_columns_str.columns.tolist()\n",
    "\n",
    "# name_columns_str.remove('Tran Date')\n",
    "\n",
    "# name_columns_str\n",
    "\n",
    "# for i in name_columns_str:\n",
    "#     print(i)\n",
    "#     df[i] = df[i].apply(lambda x: x.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro de los casos para bajar el reporte con el código de Experian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:52:43.552452Z",
     "start_time": "2024-07-05T19:52:43.547880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****\n",
      "Cuentas que cumplen condiciones a cerrar: 65\n"
     ]
    }
   ],
   "source": [
    "# Se filtran sólo las que se les bajará el reporte por parte de operaciones\n",
    "# Account Closed Successfully y Account is Already Closed son parametros validos para seguir\n",
    "df = df[(df['Closure Notes'] == 'Suplantacion')&((df['Error Description']=='Account Closed Successfully')|(df['Error Description']=='Account is Already Closed'))].reset_index(drop=True)\n",
    "accounts = tuple([f'{i}' for i in df['Account Number']])\n",
    "accounts_list = list(accounts)\n",
    "accounts_tuple = str(accounts)\n",
    "accounts_list = tuple(accounts_list)\n",
    "\n",
    "print(f\"\\n*****\\nCuentas que cumplen condiciones a cerrar: {len(accounts_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura datos desde Glue Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.030422Z",
     "start_time": "2024-07-05T19:52:43.553569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT  \n",
      "    ticket_id,\n",
      "    created_at,\n",
      "    fields_payload['numero_de_cuenta_870'] as numero_de_cuenta_870,\n",
      "    fields_payload['nombre_del_solicitante'] as nombre_del_solicitante,\n",
      "    fields_payload['tipo_de_documento'] as tipo_documento,\n",
      "    fields_payload['numero_de_documento'] as numero_de_documento,\n",
      "    fields_payload['celular_de_quien_se_comunica'] as celular_de_quien_se_comunica,\n",
      "    CONCAT(\n",
      "        RPAD(fields_payload['numero_de_cuenta_870'], 18, '0'),\n",
      "        LPAD(fields_payload['numero_de_documento'], 11, '0'), \n",
      "        (CASE \n",
      "            WHEN fields_payload['tipo_de_documento'] = 'tarjeta_cédula_de_extranjería' THEN '4' \n",
      "            WHEN fields_payload['tipo_de_documento'] = 'tarjeta_tarjeta_de_identidad' THEN '7'\n",
      "            ELSE '1' END),\n",
      "        'E') as code_experian -- genera el codigo experian con la longitud necesaria para crear el masivo por registro\n",
      "        -- E significa que se baja el reporte por que \"Se presento Fraude\"\n",
      "FROM co_delfos_servicio_raw_pdn_rl.co_zendesk_flatten_tickets\n",
      "WHERE   true\n",
      "    and ticket_form_label = 'gestion_de_fraude'\n",
      "    and fields_payload['numero_de_cuenta_870'] in ('87077511281', '87077512530', '87077511530', '87078303255', '87075165629', '87078290987', '87065254939', '87072722106', '87056440776', '87041617044', '87049075905', '87041396232', '87059064539', '87044498443', '87060026549', '87056759007', '87067243646', '87044590250', '87054765902', '87059869452', '87057689364', '87049459476', '87061040848', '87057580352', '87058906823', '87051198975', '87050584410', '87056232334', '87061921078', '87049296406', '87041267912', '87042791381', '87045856514', '87070539448', '87067527675', '87063697112', '87043978073', '87049027166', '87052025570', '87059848878', '87061787940', '87043542442', '87051383088', '87055347808', '87073578619', '87066747954', '87054106774', '87051395032', '87055136710', '87059462051', '87062500735', '87062194357', '87045059209', '87067294775', '87043093322', '87057558754', '87059552127', '87052317051', '87072884729', '87052970527', '87064489879', '87056241995', '87056233411', '87065286031', '87064655947')\n",
      "ORDER BY created_at DESC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se envian parametros para armar el query con ayuda de la plantilla.\n",
    "query_string_zendesk = SqlRender.render_template(name_template=\"consulta.sql\", \n",
    "                                                 database=\"co_delfos_servicio_raw_pdn_rl\",\n",
    "                                                 table=\"co_zendesk_flatten_tickets\", \n",
    "                                                 accounts_list = accounts_list)\n",
    "print(query_string_zendesk)\n",
    "#Se le envia el query a Athena para que retorne la información en un dataframe.\n",
    "df_athena_zendesk = AthenaQueryExecutor.execute_query(query=query_string_zendesk,\n",
    "                                                      database_athena=\"co_delfos_servicio_raw_pdn_rl\",\n",
    "                                                      workgroup_athena=\"delfos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.035328Z",
     "start_time": "2024-07-05T19:53:07.031708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros encontrados en Glue Catalog: (75, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Registros encontrados en Glue Catalog: {df_athena_zendesk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.042296Z",
     "start_time": "2024-07-05T19:53:07.037258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se eliminan duplicados, dejando solo ultimo ticket creado con diferente 870 (no duplicado)\n",
    "df_final = df_athena_zendesk.sort_values('created_at')\\\n",
    "                            .drop_duplicates(subset=['numero_de_cuenta_870'], keep='last')\\\n",
    "                            .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar archivos de salida segun la particion de cada APR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para exportar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.052317Z",
     "start_time": "2024-07-05T19:53:07.043308Z"
    }
   },
   "outputs": [],
   "source": [
    "def verificar_y_crear(nombre_archivo, carpeta, dataframe):\n",
    "    # Verificar si el archivo ya existe en la carpeta\n",
    "    if os.path.exists(os.path.join(carpeta, nombre_archivo)):\n",
    "        # Si existe, agregar \"_2\" al nombre del archivo\n",
    "        nombre_sin_extension, extension = os.path.splitext(nombre_archivo)\n",
    "        nuevo_nombre = f\"{nombre_sin_extension}_2{extension}\"\n",
    "        # Verificar si el nuevo nombre ya existe\n",
    "        while os.path.exists(os.path.join(carpeta, nuevo_nombre)):\n",
    "            # Si existe, incrementar el número al final del nombre\n",
    "            indice = nuevo_nombre.rfind(\"_\")\n",
    "            numero = int(nuevo_nombre[indice + 1: -len(extension)])\n",
    "            nuevo_nombre = f\"{nombre_sin_extension}_{numero + 1}{extension}\"\n",
    "        # Crear el nuevo archivo con el nuevo nombre\n",
    "        if extension == \".txt\":\n",
    "            dataframe.to_csv(os.path.join(carpeta, nuevo_nombre), index=False, header=False ,sep='\\t')\n",
    "        else:\n",
    "            dataframe.to_csv(os.path.join(carpeta, nuevo_nombre), index=False, sep='\\t')\n",
    "        print(f\"El archivo {nombre_archivo} ya existe. Se creó {nuevo_nombre}.\")\n",
    "        # Condicion para crear o no el archivo xlsm, ya que no todos deben quedar con .xlsx\n",
    "        if coinciden_iniciales(nombre_archivo) == False:\n",
    "            guardar_como_excel(os.path.join(carpeta, nuevo_nombre))\n",
    "        \n",
    "    else:\n",
    "        nombre_sin_extension, extension = os.path.splitext(nombre_archivo)\n",
    "        # Si no existe, crear el archivo con el nombre original\n",
    "        if extension == \".txt\":\n",
    "            dataframe.to_csv(os.path.join(carpeta, nombre_archivo), index=False, header=False, sep='\\t')\n",
    "        else:\n",
    "            dataframe.to_csv(os.path.join(carpeta, nombre_archivo), index=False, sep='\\t')\n",
    "        print(f\"Se creó el archivo {nombre_archivo}.\")\n",
    "        # Condicion para crear o no el archivo xlsm, ya que no todos deben quedar con .xlsx\n",
    "        if coinciden_iniciales(nombre_archivo) == False:\n",
    "            guardar_como_excel(os.path.join(carpeta, nombre_archivo))\n",
    "        \n",
    "def guardar_como_excel(ruta_archivo_csv):\n",
    "    # Ruta del archivo CSV o TXT\n",
    "\n",
    "    # Determina si es un archivo CSV o TXT\n",
    "    if ruta_archivo_csv.lower().endswith('.csv'):\n",
    "        # Lee el archivo CSV en un DataFrame\n",
    "        df = pd.read_csv(ruta_archivo_csv, delimiter='\\t')\n",
    "    elif ruta_archivo_csv.lower().endswith('.txt'):\n",
    "        # Lee el archivo TXT en un DataFrame, utilizando tabulaciones como delimitador\n",
    "        df = pd.read_csv(ruta_archivo_csv, delimiter='\\t')\n",
    "    else:\n",
    "        print(\"El archivo no es compatible. Debe ser un archivo CSV o TXT.\")\n",
    "        exit()\n",
    "\n",
    "    # Genera el nombre del archivo Excel\n",
    "    archivo_destino = ruta_archivo_csv[:-4] + '.xlsx'\n",
    "\n",
    "    # Guarda el DataFrame como un archivo Excel\n",
    "    df.to_excel(archivo_destino, index=False)\n",
    "\n",
    "    # print(f'Se ha creado el archivo \"{archivo_destino}\" en formato XLSX.')\n",
    "\n",
    "def coinciden_iniciales(name):\n",
    "    lista = [\"Bloqueo_2\", \"df_final\", \"APRCierre\"]\n",
    "    for elemento in lista:\n",
    "        if name.startswith(elemento):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def eliminar_APR(ruta_archivo):\n",
    "    if os.path.exists(ruta_archivo):\n",
    "        # Eliminar el archivo\n",
    "        os.remove(ruta_archivo)\n",
    "        print(f\"El archivo en la ruta {ruta_archivo} ha sido eliminado exitosamente.\")\n",
    "    else:\n",
    "        print(f\"No se encontró ningún archivo en la ruta {ruta_archivo}.\")\n",
    "\n",
    "def copiar_y_pegar_archivo(origen, destino):\n",
    "    # Comprobar si el archivo ya existe en la carpeta de destino\n",
    "    if os.path.exists(destino):\n",
    "        nombre_archivo = os.path.basename(origen)\n",
    "        # Obtener el nombre del archivo sin extensión\n",
    "        nombre_base, extension = os.path.splitext(nombre_archivo)\n",
    "        # Buscar un nombre único para el nuevo archivo\n",
    "        indice = 1\n",
    "        while os.path.exists(os.path.join(destino, nombre_archivo)):\n",
    "            nombre_archivo = f\"{nombre_base}_{indice}{extension}\"\n",
    "            indice += 1\n",
    "        destino = os.path.join(destino, nombre_archivo)\n",
    "    # Copiar y pegar el archivo\n",
    "    shutil.copy2(origen, destino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exportar file APR.txt\n",
    "- Tener en cuenta exportar con el nombre de \"masivo_APR_<fecha_APR_Original>.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.065334Z",
     "start_time": "2024-07-05T19:53:07.053384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creó el archivo APRCierre_20250513.csv.\n",
      "Se creó el archivo Bloqueo_20250513.txt.\n",
      "Se creó el archivo Bloqueo_20250513.csv.\n",
      "Se creó el archivo df_final_20250513.csv.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extraer particion del nombre del archivo original (insumo)\n",
    "\"\"\"\n",
    "\n",
    "# Aplicar la expresión regular al nombre ORIGINAL del archivo\n",
    "match = re.search(regex_partition, archivos_excel[0])\n",
    "\n",
    "# Verificar si se encontró el grupo de captura (particion)\n",
    "if match:\n",
    "    partition = match.group(1)\n",
    "else:\n",
    "    print(\"No se encontró el número en el nombre del archivo.\")\n",
    "\n",
    "\"\"\"\n",
    "Almacenar en las particiones del folder data/output\n",
    "\"\"\"\n",
    "\n",
    "# Directorio data output\n",
    "dir_data_output = os.path.join(dir_project, 'co_gestionFraude_eliminacionMasivaExperian/data/output')\n",
    "\n",
    "# Concatenar ruta destino con particion\n",
    "folder_destino = os.path.join(dir_data_output, f'{partition[0:4]}/{partition[4:6]}/{partition[6:8]}')\n",
    "\n",
    "# Si el folder no existe, crearlo\n",
    "if not os.path.exists(folder_destino):\n",
    "    os.makedirs(folder_destino)\n",
    "\n",
    "# Archivo original (name + dir)\n",
    "name_original_file_input = f'APRCierre_{partition}.csv'\n",
    "dir_original_file_input = os.path.join(folder_destino, name_original_file_input)\n",
    "\n",
    "# Archivo procesado txt masivo experian (name + dir)\n",
    "name_txt_file_output = f'Bloqueo_{partition}.txt'\n",
    "dir_xlsx_file_output = os.path.join(folder_destino, name_txt_file_output)\n",
    "\n",
    "# Archivo csv masivo experian procesado (name + dir)\n",
    "name_csv_file_output = f'Bloqueo_{partition}.csv'\n",
    "dir_csv_file_output = os.path.join(folder_destino, name_csv_file_output)\n",
    "\n",
    "# Archivo csv masivo experian procesado (name + dir)\n",
    "name_df_final_csv_file_output = f'df_final_{partition}.csv'\n",
    "dir_df_final_csv_file_output = os.path.join(folder_destino, name_csv_file_output)\n",
    "\n",
    "# Mover el archivo original\n",
    "\n",
    "copiar_y_pegar_archivo(file_input_path, folder_destino)\n",
    "# shutil.copy(file_input_path, dir_original_file_input)\n",
    "\n",
    "# # Creando archivo APR original\n",
    "verificar_y_crear(name_original_file_input, folder_destino, df_copy)\n",
    "# Creando archivo de salida masivo .txt\n",
    "verificar_y_crear(name_txt_file_output, folder_destino, df_final.code_experian)\n",
    "# Creando archivo de salida masivo .csv\n",
    "verificar_y_crear(name_csv_file_output, folder_destino, df_final.code_experian)\n",
    "# Creando archivo df_final\n",
    "verificar_y_crear(name_df_final_csv_file_output, folder_destino, df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exportar el file de historico en formato .xlsx\n",
    "- Tener en cuenta exportar con el nombre de \"masivo_APR_<fecha_APR_Original>.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.100663Z",
     "start_time": "2024-07-05T19:53:07.066238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron tickets para los siguientes números de cuenta: ['87077511281', '87077512530', '87075165629']\n",
      "Se creó el archivo Bloqueo_hist_20250513.csv.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Limpieza datos\n",
    "\"\"\"\n",
    "# Se muestran los Errores de tipificación en 870 comparando el 870 del APR con el del ticket\n",
    "df_final['numero_de_cuenta_870'] = df_final['numero_de_cuenta_870'].astype(np.int64) # Se comenta esta linea porque esta generando un error el tipado de la columna y no permite cruzar nada en el join.\n",
    "error_accounts_list = []\n",
    "for i in accounts_list:\n",
    "    if i not in list(df_final['numero_de_cuenta_870'].astype(str)):\n",
    "        error_accounts_list.append(i)\n",
    "print(f\"No se encontraron tickets para los siguientes números de cuenta: {error_accounts_list}\")\n",
    "\n",
    "# Se genera archivo para llevar registro histórico\n",
    "df_hist = df.merge(df_final, how='inner', \n",
    "                   left_on='Account Number', \n",
    "                   right_on='numero_de_cuenta_870')\n",
    "\n",
    "reporte = creacion_archivo_historico(df_hist)\n",
    "\n",
    "\"\"\"\n",
    "Exportar archivo .xlsx\n",
    "\"\"\"\n",
    "name_xlsx_file_output = f'Bloqueo_hist_{partition}.csv'\n",
    "verificar_y_crear(name_xlsx_file_output, folder_destino, reporte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T19:53:07.104376Z",
     "start_time": "2024-07-05T19:53:07.101628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo en la ruta c:\\Users\\disanchez\\OneDrive - PersonalSoft S.A.S\\Analítica\\Github\\co_gestionFraude_eliminacionMasivaExperian/data/input\\APRCIERREMLZ2025051300.xlsx ha sido eliminado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Eliminando APR de la ruta input\n",
    "eliminar_APR(file_input_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nequi_experian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
